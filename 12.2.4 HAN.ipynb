{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 超参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 200\n",
    "hidden_dim = 16\n",
    "l2_coef = 5e-4\n",
    "dataset = 'cora'\n",
    "dataset_path = './examples/gcn/'\n",
    "best_model_path = './'\n",
    "self_loops = 1\n",
    "gpu = -1\n",
    "if gpu >= 0:\n",
    "    tlx.set_device(\"GPU\", gpu)\n",
    "else:\n",
    "    tlx.set_device(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 数据集加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gammagl.transforms as T\n",
    "from gammagl.datasets import IMDB\n",
    "from gammagl.utils import mask_to_index, set_device\n",
    "\n",
    "metapaths = [[('movie', 'actor'), ('actor', 'movie')],\n",
    "    [('movie', 'director'), ('director', 'movie')]]\n",
    "transform = T.AddMetaPaths(metapaths=metapaths, drop_orig_edges=True,\n",
    "drop_unconnected_nodes=True)\n",
    "dataset = IMDB(args.dataset_path, transform=transform)\n",
    "graph = dataset[0]\n",
    "y = graph['movie'].y\n",
    "\n",
    "train_idx = mask_to_index(graph['movie'].train_mask, )\n",
    "test_idx = mask_to_index(graph['movie'].test_mask)\n",
    "val_idx = mask_to_index(graph['movie'].val_mask)\n",
    "\n",
    "edge_index_dict = {}\n",
    "if tlx.BACKEND == 'tensorflow':\n",
    "    edge_index_dict = graph.edge_index_dict\n",
    "else:\n",
    "    edge_index_dict[('movie', 'metapath_0', 'movie')] = tlx.convert_to_tensor(\n",
    "        graph.edge_index_dict[('movie', 'metapath_0', 'movie')], dtype=tlx.int64)\n",
    "    edge_index_dict[('movie', 'metapath_1', 'movie')] = tlx.convert_to_tensor(\n",
    "        graph.edge_index_dict[('movie', 'metapath_1', 'movie')], dtype=tlx.int64)\n",
    "data = {\n",
    "\"x_dict\": graph.x_dict,\n",
    "\"y\": y,\n",
    "\"edge_index_dict\": edge_index_dict,\n",
    "\"train_idx\": train_idx,\n",
    "\"test_idx\": test_idx,\n",
    "\"val_idx\": val_idx,\n",
    "\"num_nodes_dict\": {'movie': graph['movie'].num_nodes},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 卷积层构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorlayerx as tlx\n",
    "from tensorlayerx.nn import Module, Sequential, ModuleDict, Linear, Tanh\n",
    "from gammagl.layers.conv import MessagePassing\n",
    "from gammagl.layers.conv import GATConv\n",
    "\n",
    "# SemAttAggr类实现了一个聚合模块，用于通过注意力机制聚合来自不同邻居节点的特征。\n",
    "class SemAttAggr(Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.project = Sequential(\n",
    "            Linear(in_features=in_size, out_features=hidden_size),\n",
    "            Tanh(),\n",
    "            Linear(in_features=hidden_size, out_features=1, b_init=None)\n",
    "        ) # 定义了一个线性投影，通过Tanh激活函数进行变换。\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = tlx.reduce_mean(self.project(z), axis=1) \n",
    "        beta = tlx.softmax(w, axis=0)\n",
    "        beta = tlx.expand_dims(beta, axis=-1)\n",
    "        return tlx.reduce_sum(beta * z, axis=0) # 加权聚合特征\n",
    "\n",
    "class HANConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 metadata,\n",
    "                 heads=1,\n",
    "                 negative_slope=0.2):\n",
    "        super().__init__()\n",
    "        if not isinstance(in_channels, dict):\n",
    "            in_channels = {node_type: in_channels for node_type in metadata[0]}\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.metadata = metadata\n",
    "        self.heads = heads\n",
    "        self.negetive_slop = negative_slope\n",
    "        self.gat_dict = ModuleDict({}) # 用来初始化每种边类型的GATConv层。\n",
    "        for edge_type in metadata[1]:\n",
    "            src_type, _, dst_type = edge_type\n",
    "            edge_type = '__'.join(edge_type)\n",
    "            self.gat_dict[edge_type] = GATConv(in_channels=in_channels[src_type],\n",
    "                                               out_channels=out_channels,\n",
    "                                               heads=heads,\n",
    "                                               concat=True)\n",
    "        self.sem_att_aggr = SemAttAggr(in_size=out_channels*heads,\n",
    "                                       hidden_size=out_channels) # 用于计算每个节点类型的特征加权和来聚合信息\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, num_nodes_dict):\n",
    "        out_dict = {}  # 用来存储每种类型节点的输出结果。\n",
    "        for node_type, x_node in x_dict.items():\n",
    "            out_dict[node_type] = []\n",
    "        for edge_type, edge_index in edge_index_dict.items(): # 对于每种边类型，使用相应的GATConv层来计算节点特征。\n",
    "            src_type, _, dst_type = edge_type\n",
    "            edge_type = '__'.join(edge_type)\n",
    "            out = self.gat_dict[edge_type](x_dict[src_type],\n",
    "                                           edge_index,\n",
    "                                           num_nodes = num_nodes_dict[dst_type])\n",
    "            out = tlx.relu(out)\n",
    "            out_dict[dst_type].append(out)\n",
    "      # 在对所有边类型的输出计算完成后，对每个节点类型的输出进行聚合。\n",
    "        for node_type, outs in out_dict.items():\n",
    "            outs = tlx.stack(outs)\n",
    "            out_dict[node_type] = self.sem_att_aggr(outs)\n",
    "        return out_dict\n",
    "    \n",
    "class HAN(tlx.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 metadata,\n",
    "                 hidden_channels=128,\n",
    "                 heads=8):\n",
    "        super().__init__(name=None)\n",
    "        self.han_conv = HANConv(in_channels,\n",
    "                                hidden_channels,\n",
    "                                metadata,\n",
    "                                heads=heads)\n",
    "        self.lin = tlx.nn.Linear(in_features=hidden_channels*heads, \n",
    "                                 out_features=out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, num_nodes_dict):\n",
    "        x = self.han_conv(x_dict, edge_index_dict, num_nodes_dict)\n",
    "        out = {}\n",
    "        for node_type, _ in num_nodes_dict.items():\n",
    "            out[node_type] = self.lin(x[node_type]) # 通过一个全连接层将结果映射到输出空间\n",
    "        return out\n",
    "net = HAN(\n",
    "        in_channels=graph.x_dict['movie'].shape[1],\n",
    "        out_channels=3,\n",
    "        metadata=graph.metadata(),\n",
    "        drop_rate=args.drop_rate,\n",
    "        hidden_channels=args.hidden_dim,\n",
    "        heads=args.heads,\n",
    "        name='han',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorlayerx.model import WithLoss, TrainOneStep \n",
    "class SemiSpvzLoss(WithLoss):\n",
    "    def __init__(self, net, loss_fn):\n",
    "        super(SemiSpvzLoss,   self).__init__(backbone=net, loss_fn=loss_fn)\n",
    "    def forward(self, data, y):\n",
    "        logits = self.backbone_network(data['x'],\n",
    "        data['edge_index'],\n",
    "        None,\n",
    "        data['num_nodes']\n",
    "        )\n",
    "        # 根据输入的节点特征、边的连接信息等数据计算出模型的输出（logits）\n",
    "        train_logits = tlx.gather(logits, data['train_idx'])  \n",
    "        # 通过tlx.gather从标签中选择出训练集的真实标签 \n",
    "        train_y = tlx.gather(data['y'], data['train_idx']) \n",
    "        loss = self._loss_fn(train_logits, train_y)\n",
    "        return loss\n",
    "\n",
    "train_weights = net.trainable_weights\n",
    "loss_func = SemiSpvzLoss(net,   tlx.losses.softmax_cross_entropy_with_logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 设置优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tlx.optimizers.Adam(lr=args.lr, weight_decay=args.l2_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 模型评估函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(logits, y, metrics):\n",
    "    metrics.update(logits, y)\n",
    "    rst = metrics.result()\n",
    "    metrics.reset()\n",
    "    return rst\n",
    "metrics = tlx.metrics.Accuracy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 模型训练流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_step = TrainOneStep(loss_func, optimizer, train_weights)\n",
    "best_val_acc = 0\n",
    "for epoch in range(args.n_epoch):\n",
    "    net.set_train()\n",
    "    train_loss = train_one_step(data, graph.y) # 进行每轮训练\n",
    "    net.set_eval()\n",
    "    logits = net(data['x'], data['edge_index'], None, data['num_nodes']) # 执行模型的前向传播计算，生成预测的输出（logits）\n",
    "    val_logits = tlx.gather(logits, data['val_idx'])\n",
    "    val_y = tlx.gather(data['y'], data['val_idx'])\n",
    "    val_acc = calculate_acc(val_logits, val_y, metrics)# 计算验证集上的准确率。val_logits是验证集的预测值，val_y是验证集的真实标签，metrics是评估标准\n",
    "    print(\"Epoch [{:0>3d}] \".format(epoch+1)\\\n",
    "        + \" train loss: {:.4f}\".format(train_loss.item())\\\n",
    "        + \" val acc: {:.4f}\".format(val_acc))\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # 保留验证集上表现最好的模型参数,作为测试集采用的模型参数\n",
    "        net.save_weights(args.best_model_path+net.name+\".npz\", format='npz_dict')\n",
    "\n",
    "net.load_weights(args.best_model_path+net.name+\".npz\", format='npz_dict')\n",
    "net.set_eval()\n",
    "logits = net(data['x'], data['edge_index'], None, data['num_nodes'])\n",
    "test_logits = tlx.gather(logits, data['test_idx'])\n",
    "test_y = tlx.gather(data['y'], data['test_idx'])\n",
    "test_acc = calculate_acc(test_logits, test_y, metrics)\n",
    "print(\"Test acc: {:.4f}\".format(test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
