{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 超参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 200\n",
    "hidden_dim = 16\n",
    "l2_coef = 5e-4\n",
    "dataset = 'cora'\n",
    "dataset_path = './examples/gcn/'\n",
    "best_model_path = './'\n",
    "self_loops = 1\n",
    "gpu = -1\n",
    "if gpu >= 0:\n",
    "    tlx.set_device(\"GPU\", gpu)\n",
    "else:\n",
    "    tlx.set_device(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 数据集处理与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "g = dataset[0]\n",
    "model = DeepWalk(g)\n",
    "dataloader = DataLoader(torch.arange(g.num_nodes()), batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=model.sample)\n",
    "train_mask = g.ndata['train_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "\n",
    "X = model.node_embed.weight.detach()\n",
    "y = g.ndata['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. DeepWalk模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWalk(nn.Module):\n",
    "    def __init__(self, g, emb_dim=128, walk_length=40, window_size=5,\n",
    "        neg_weight=1, negative_size=5, sparse=True):\n",
    "        super().__init__()\n",
    "        self.g = g\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.walk_length = walk_length\n",
    "        self.neg_weight = neg_weight\n",
    "        self.negative_size = negative_size\n",
    "        num_nodes = g.num_nodes()\n",
    "        # 中心节点嵌入\n",
    "        self.node_embed = nn.Embedding(num_nodes, emb_dim, sparse=sparse)\n",
    "        self.context_embed = nn.Embedding(num_nodes, emb_dim, sparse=sparse)\n",
    "        self.reset_parameters()\n",
    "        # 获得正例列表索引对。\n",
    "        idx_list_src = []\n",
    "        idx_list_dst = []\n",
    "        for i in range(walk_length):\n",
    "            for j in range(max(0, i - window_size), i):\n",
    "                idx_list_src.append(j)\n",
    "                idx_list_dst.append(i)\n",
    "            for j in range(i + 1, min(walk_length, i + 1 + window_size)):\n",
    "                idx_list_src.append(j)\n",
    "                idx_list_dst.append(i)\n",
    "\n",
    "        self.idx_list_src = torch.LongTensor(idx_list_src)\n",
    "        self.idx_list_dst = torch.LongTensor(idx_list_dst)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 初始化权重\n",
    "        init_range = 1.0 / self.emb_dim\n",
    "        init.uniform_(self.node_embed.weight.data, -init_range, init_range)\n",
    "        init.constant_(self.context_embed.weight.data, 0)\n",
    "\n",
    "    def sample(self, indices):\n",
    "        return random_walk(self.g, indices, length=self.walk_length - 1)[0]\n",
    "\n",
    "    def forward(self, batch_walk):\n",
    "        batch_size = len(batch_walk)\n",
    "        device = batch_walk.device\n",
    "\n",
    "        # 获取批次节点和上下文节点的嵌入\n",
    "        batch_node_embed = self.node_embed(batch_walk).view(-1, self.emb_dim)\n",
    "        batch_context_embed = self.context_embed(batch_walk).view(\n",
    "            -1, self.emb_dim\n",
    "        )\n",
    "\n",
    "        # 计算正样本的索引偏移\n",
    "        batch_idx_list_offset = torch.arange(batch_size) * self.walk_length\n",
    "        batch_idx_list_offset = batch_idx_list_offset.unsqueeze(1)\n",
    "        idx_list_src = batch_idx_list_offset + self.idx_list_src.unsqueeze(0)\n",
    "        idx_list_dst = batch_idx_list_offset + self.idx_list_dst.unsqueeze(0)\n",
    "        idx_list_src = idx_list_src.view(-1).to(device)\n",
    "        idx_list_dst = idx_list_dst.view(-1).to(device)\n",
    "\n",
    "        # 获取正样本的嵌入\n",
    "        pos_src_emb = batch_node_embed[idx_list_src]\n",
    "        pos_dst_emb = batch_context_embed[idx_list_dst]\n",
    "\n",
    "        # 获取负样本的嵌入\n",
    "        neg_idx_list_src = idx_list_dst.unsqueeze(1) + torch.zeros(\n",
    "            self.negative_size\n",
    "        ).unsqueeze(0).to(device)\n",
    "        neg_idx_list_src = neg_idx_list_src.view(-1)\n",
    "        neg_src_emb = batch_node_embed[neg_idx_list_src.long()]\n",
    "\n",
    "        neg_idx_list_dst = list(range(batch_size * self.walk_length)) * (\n",
    "            self.negative_size * self.window_size * 2\n",
    "        )\n",
    "        random.shuffle(neg_idx_list_dst)\n",
    "        neg_idx_list_dst = neg_idx_list_dst[: len(neg_idx_list_src)]\n",
    "        neg_idx_list_dst = torch.LongTensor(neg_idx_list_dst).to(device)\n",
    "        neg_dst_emb = batch_context_embed[neg_idx_list_dst]\n",
    "\n",
    "        # 计算正样本得分\n",
    "        pos_score = torch.sum(torch.mul(pos_src_emb, pos_dst_emb), dim=1)\n",
    "        pos_score = torch.clamp(pos_score, max=6, min=-6)\n",
    "        pos_score = torch.mean(-F.logsigmoid(pos_score))\n",
    "        # 计算负样本得分\n",
    "        neg_score = torch.sum(torch.mul(neg_src_emb, neg_dst_emb), dim=1)\n",
    "        neg_score = torch.clamp(neg_score, max=6, min=-6)\n",
    "        neg_score = (\n",
    "            torch.mean(-F.logsigmoid(-neg_score))\n",
    "            * self.negative_size\n",
    "            * self.neg_weight\n",
    "        )\n",
    "        # 取正样本得分和负样本得分作为损失\n",
    "        return torch.mean(pos_score + neg_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 预测器实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测器\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, out_feats)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 设置优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SparseAdam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. DeepWalk训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_walk in dataloader:\n",
    "        loss = model(batch_walk)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"DeepWalk Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 预测器训练与推理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(in_feats=emb_dim,\n",
    "                        hidden_size=hidden_dim,\n",
    "                        out_feats=g.ndata['label'].max().item() + 1)\n",
    "optimizer_mlp = optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "# 训练流程\n",
    "for epoch in range(epochs):\n",
    "    mlp_model.train()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    # 使用DeepWalk嵌入作为特征输入\n",
    "    output = mlp_model(X[train_mask])\n",
    "    loss = F.cross_entropy(output, y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_mlp.step()\n",
    "print(f'MLP Epoch {epoch}, Loss: {loss.item()}')\n",
    "# 推理流程\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = mlp_model(X[test_mask])\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    accuracy = (predicted == y[test_mask]).sum().item() / len(y[test_mask])\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
